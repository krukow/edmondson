{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spidercats: psychological safety and generative culture readout\n",
    "Note: \"Spidercats\" is a fictional team-name used for the purpose of this example report.\n",
    "\n",
    "# Introduction\n",
    "In Nov 2020, we’ve measured two aspects of the work environment and culture in the Spidercats engineering team: (i) the degree of psychological safety; and (ii) the degree to which our culture can be considered “generative.” This document provides an overview of these concepts as well as the results of the surveys used to measure these.\n",
    "\n",
    "## What is psychological safety and generative culture?\n",
    "**Psychological safety** expresses the degree to which individuals on our team perceive the work environment as safe to take interpersonal risk, e.g., speaking up with non-confirmative opinions in meetings, asking questions, and asking for help. [Research by Amy Edmondson](https://www.amazon.com/Fearless-Organization-Psychological-Workplace-Innovation/dp/1119477247) as well as “[Project Aristotle](https://rework.withgoogle.com/print/guides/5721312655835136/)”, a large study from Google, has shown that a high degree of psychological safety is an important aspect of effective teams.\n",
    "\n",
    "Related to this is the concept of a **generative culture** which was popularized by the [Accelerate book](https://www.amazon.com/Accelerate-Software-Performing-Technology-Organizations/dp/1942788339) and The “[State of DevOps](https://www.devops-research.com/research.html)” research reports. This model of culture considers culture as being on a spectrum ranging from “pathological” (power-oriented) to “bureaucratic” (rule-oriented) to “generative” (performance-oriented) cultures. In Accelerate it was shown that organizations with generative culture have higher levels of job satisfaction, as well as achieve higher levels of organizational performance (ability to deliver on the goals they set).\n",
    "\n",
    "## The survey\n",
    "We’ve measured the concepts of psychological safety and generative culture within the Spidercats team using a [set of valid and reliable survey questions](https://forms.gle/ByXsuvB614vopVVF7) taken from the research of Amy Edmondson (The Fearless Organization) and Forsgren et. al (Accelerate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the analysis\n",
    "We'll use the [Edmondson library](https://github.com/krukow/edmondson/) for analyzing the survey results stored in a Google sheets document.\n",
    "\n",
    "The following code block prepares the library for usage with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(ns google-sheets-example\n",
    "  (:require [edmondson.google-api :as api]\n",
    "            [edmondson.survey-analysis :as analysis]\n",
    "            [edmondson.survey-model :as model]\n",
    "            [edmondson.config :as cfg]\n",
    "            [edmondson.reports :as reports]\n",
    "            [clojure.string :as str])\n",
    "  (:use     [clojupyter.display :only (hiccup-html html latex markdown render-mime)]))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting survey answers to quantitative data\n",
    "The survey has four parts. There are two quantitative (\"[psychometric](https://en.wikipedia.org/wiki/Psychometrics)\") constructs: \"Psychological safety\", \"Generative culture\", and two qualitative, open-ended topics \"Psychological safety domains\", \"Open-ended feedback\".\n",
    "\n",
    "For the quantitative constructs, we'll use a 7pt Likert scale for scoring each of the questions in the construct. \n",
    "\n",
    "The following snipplet defines the scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(def agree-disagree-7-pt-params ;; default scoring for 7pt Likert scale\n",
    "  {\"Strongly agree\" 7\n",
    "   \"Agree\" 6\n",
    "   \"Somewhat agree\" 5\n",
    "   \"Neutral\" 4\n",
    "   \"Somewhat disagree\" 3\n",
    "   \"Disagree\" 2\n",
    "   \"Strongly Disagree\" 1})\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the default score for each answer. However, for some questions Strongly agree is the \"worst\" answer and these are scored in reverse. The following two scoring functions represent the default scale and the negative (reverse scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(defn default-scale \"identity scoring transformation\" [a _] a)\n",
    "\n",
    "(defn negative-scale\n",
    "  \"Scoring transform for reverse-scored likert questions\"\n",
    "  [a likert-scale]\n",
    "  (- (inc (count likert-scale)) a))\n",
    "\n",
    ";; Example\n",
    "(let [answer \"Strongly agree\"\n",
    "      default-score (get agree-disagree-7-pt-params answer)]\n",
    "  (negative-scale default-score agree-disagree-7-pt-params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model of the survey\n",
    "The survey model is an important concept in the Edmondson library. The model defines the \"topics\" being surveyed (sometimes called \"constructs\"). Each topic declares how it will be processed in the analysis (e.g. scored quantitatively).\n",
    "\n",
    "For example, the \"Psychological safety\" construct declares: \n",
    "1. a name (\"Psychological safety\");\n",
    "2. a unique set of survey question, e.g. \"I am able to bring up problems and tough issues.\" (implicitly prefixed with \"How much do you agree or disagree with the following statement(s)?\");\n",
    "3. a default scoring map (agree-disagree-7-pt-params) and a default scale (default-scale).\n",
    "4. optionally, each question can override the defaults and state that it must be scored using another scale (e.g. \"I worry that mistakes will be held against me.\" is scored with the `negative-scale` since \"Strongly agree\" is the worst possible answer to this question).\n",
    "\n",
    "As you can see below, psychological safety and generative culture are scored using a 7-pt Likert scale with some questions reverse scored.\n",
    "\n",
    "For \"Psychological safety domains\" and \"Open-ended feedback\" we don't do quantitative analysis. For these we simply gather peoples responses (the `:groups` and `:verbatims` keywords declare this, and the usage is explained later in this document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{:scoring\n",
      " {\"Strongly agree\" 7,\n",
      "  \"Agree\" 6,\n",
      "  \"Somewhat agree\" 5,\n",
      "  \"Neutral\" 4,\n",
      "  \"Somewhat disagree\" 3,\n",
      "  \"Disagree\" 2,\n",
      "  \"Strongly Disagree\" 1},\n",
      " :scale #function[google-sheets-example/default-scale]}\n"
     ]
    }
   ],
   "source": [
    "(def example-model\n",
    "  {\n",
    "   \"Psychological safety\"\n",
    "   {:scoring agree-disagree-7-pt-params ;; default scoring with 7p likert scale\n",
    "    :scale default-scale ;; default scale is normal scale\n",
    "    :questions [[\"I worry that mistakes will be held against me.\"\n",
    "                 ;; since Strongly agree is quite bad here.\n",
    "                 ;; this questions is scored in reverse, so we override default scale:\n",
    "                 {:scale negative-scale}]\n",
    "                \"I am able to bring up problems and tough issues.\" ;; defaults\n",
    "                [\"People in the team sometimes reject others for being different.\"\n",
    "                 {:scale negative-scale}]\n",
    "                \"It is safe to take a risk within the team.\"\n",
    "                [\"I find it difficult to ask other members of the team for help.\"\n",
    "                 {:scale negative-scale}]\n",
    "                \"No one in the team would deliberately act in a way that undermines my efforts.\"\n",
    "                \"My unique skills and talents are valued and utilized in this team.\"]}\n",
    "\n",
    "   \"Generative culture\"\n",
    "   {:scoring agree-disagree-7-pt-params\n",
    "    :scale default-scale\n",
    "    :questions [\"No-one is punished for delivering news of failure or other bad news.\"\n",
    "                \"Responsibilities are shared (you hear mostly \\\"this is our responsibility\\\" vs \\\"this is not my responsibility\\\")\"\n",
    "                \"Cross functional and cross-team collaboration is encouraged and rewarded.\"\n",
    "                \"People on our team welcome new ideas, regardless of source and seniority.\"\n",
    "                \"Failure causes inquiry (failures are investigated, not ignored or hidden)\"\n",
    "                \"Failures are treated primarily as opportunities to improve the system, processes or team.\"\n",
    "                \"New information is actively sought out.\"]}\n",
    "\n",
    "   \"Psychological safety domains\"\n",
    "   {:groups [\"For you personally, choose the three topics which you find the most uncomfortable, or where you'd least want to bring up your concerns or have a conversation about.\"]}\n",
    "   ;; This allows us to group responses based on how people check\n",
    "   ;; boxes in this question. Also makes it easier to count which\n",
    "   ;; boxes get the most checks.\n",
    "\n",
    "   \"Open-ended feedback\"\n",
    "   ;; These are not analyzed quantitatively\n",
    "   {:verbatims [\"If you could change one team process, or one thing about how the team works, what would you change and why?\"\n",
    "                \"What is something the team does really well? Something that would make you very disappointed if the team stopped doing or something that makes you proud to be part of the team.\"]}\n",
    "\n",
    "   })\n",
    "\n",
    "(def model-index ;; generate an index of questions\n",
    "  (model/index-questions example-model))\n",
    "\n",
    ";; Example of using the model-index to easily looking up a question by it's unique identifier\n",
    "(-> model-index\n",
    "    (get \"It is safe to take a risk within the team.\")\n",
    "    clojure.pprint/pprint);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and score the responses\n",
    "With the survey model declared, it is time to gather the responses and analyze them according to the survey model.\n",
    "\n",
    "This code defines where we can find the survey results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    ";; Assuming the survey results are stored in the Google sheet: \n",
    ";; https://docs.google.com/spreadsheets/d/1QkBeMNGfsHHga85c-UsLAwnpmz7QyhvFK_n31CzDe7c/edit?usp=sharing\n",
    ";; which is connected automatically to the Google form.\n",
    "\n",
    ";; Define where to find the results:\n",
    "(def spreadsheetId \"1QkBeMNGfsHHga85c-UsLAwnpmz7QyhvFK_n31CzDe7c\")\n",
    "(def tab-name \"Form Responses 1\")\n",
    "(def spreadsheet-range (str tab-name \"!B:R\")) ;; this is where the answers are\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fetch and score the responses using the Edmondson library helper functions:\n",
    "\n",
    "*Note: This will take a bit of time to evaluate since it makes REST API calls the Google Sheets API.\n",
    "\n",
    "(You may have to adjust the path to `credentials.json` and `tokens`; paths are relative to this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syntax error compiling at (REPL:12:3).\n",
      "No such var: api/normalize-responses\n",
      "Execution error (IllegalArgumentException) at google-sheets-example/eval6036 (REPL:21).\n",
      "Don't know how to create ISeq from: clojure.lang.Var$Unbound\n"
     ]
    },
    {
     "ename": "class java.lang.IllegalArgumentException",
     "evalue": "",
     "execution_count": 6,
     "output_type": "error",
     "status": "error",
     "traceback": [
      "core.clj:   139 clojure.core$seq__5467/invokeStatic",
      "core.clj:  2763 clojure.core$map$fn__5935/invoke",
      "core.clj:   139 clojure.core$seq__5467/invokeStatic",
      "core.clj:  2763 clojure.core$map$fn__5935/invoke",
      "core.clj:    55 clojure.core$first__5449/invokeStatic"
     ]
    }
   ],
   "source": [
    "(binding [cfg/*credentials-json-path-override* \"../../credentials.json\"]\n",
    "    (def survey-results \n",
    "        (api/eval-range {:token-directory \"../../tokens\"}\n",
    "                        spreadsheetId \n",
    "                        spreadsheet-range)))\n",
    "\n",
    "(def survey-questions (first survey-results)) ;; first row of sheet are the questions\n",
    "(def survey-answers (rest survey-results)) ;; first row of sheet are the questions, the rest are the answers\n",
    "\n",
    "\n",
    "(def normalized-responses\n",
    "  (api/normalize-responses survey-questions survey-answers))\n",
    "\n",
    "(def scored-responses (analysis/score-responses\n",
    "                       model-index\n",
    "                       normalized-responses))\n",
    "\n",
    ";; Example of a scored response:\n",
    "(->> scored-responses\n",
    "     first\n",
    "     (render-mime \"application/json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now what?\n",
    "We've done all of the following:\n",
    "1. defined our survey model, \n",
    "2. fetched all the responses, \n",
    "3. scored each response according to the model. \n",
    "\n",
    "We've done all of this with a few lines of code with the help of the Edmondson library.\n",
    "\n",
    "Next we'll perform a data analysis of the survey results, and summarize some key findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The results: Psychological safety\n",
    "\n",
    "## What is psychological safety and why is it important?\n",
    "Psychological safety expresses the degree to which individuals on our team perceive the work environment as safe to take interpersonal risk, e.g., speaking up with non-confirmative opinions in meetings, asking questions, and asking for help. \n",
    "\n",
    "Psychological safety is critical for creating inclusive and diverse work environments. When people fear that they will be blamed or judged they protect themselves; after all it is safer to be quiet than to speak up. When psychological safety is lacking we may hesitate to disagree with leadership, point out a problem, or pretend that errors did not happen.\n",
    "\n",
    "## Data analysis: Psychological safety\n",
    "The Spidercats engineering team (including engineering managers but excluding eng. director) was surveyed in Nov 2020.\n",
    "\n",
    "### Participation rate\n",
    "We got the following results and participation rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Participation rate: 0,0% (0/22 respondents)"
     ]
    }
   ],
   "source": [
    "(def total-team-size 22) ;; update this to reflect team size\n",
    "(def num-responses (count scored-responses))\n",
    "(def participation-rate (/ num-responses (float total-team-size)))\n",
    "(printf \"\\nParticipation rate: %.1f%% (%d/%d respondents)\" (* participation-rate 100) num-responses total-team-size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct scoring\n",
    "Psychological safety (PS) is scored using a 7pt Likert scale from “Strongly disagree – Strongly agree”.\n",
    "\n",
    "Each question is scored between 1 and 7, where 1 is the worst possible answer and 7 is the best possible answer. (Sometimes “Strongly disagree” is the best possible answer.)\n",
    "\n",
    "There are a total of 7 questions that are scored for PS, and these are summed up so each response is assigned a score between 7 and 49:\n",
    "\n",
    "- The worst possible total score for a single response is 7\n",
    "- The best possible total score for a single response is 49\n",
    "- A score over 40 is considered very good.\n",
    "\n",
    "When evaluating psychological safety for a group of people, we look at the distribution of scores for each respondent.\n",
    "\n",
    "## Overall psychological safety scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>The average psychological safety score is:  (in the range [7,49]).</p>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def aggregate-results\n",
    "  (analysis/aggregate-scores example-model scored-responses))\n",
    "\n",
    "(def psych-safety-results (get aggregate-results \"Psychological safety\"))\n",
    "(def mean-psych-safety (get-in psych-safety-results [:construct-stats :score-total]))\n",
    "\n",
    "(hiccup-html \n",
    "    [:p \n",
    "     \"The average psychological safety score is: \"\n",
    "     (reports/fmt-number mean-psych-safety)\n",
    "     \" (in the range [7,49]).\"\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v3+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.json",
       "data": {
        "values": [
         {
          "good": 40,
          "mean": null
         }
        ]
       },
       "encoding": {
        "x": {
         "field": "mean",
         "scale": {
          "domain": [
           7,
           49
          ]
         },
         "type": "quantitative"
        }
       },
       "mark": {
        "type": "bar"
       }
      },
      "image/png": "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"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(render-mime \"application/vnd.vegalite.v3+json\" \n",
    "             {\n",
    "                  :$schema \"https://vega.github.io/schema/vega-lite/v4.json\",\n",
    "                  :data {:values [{:mean mean-psych-safety :good 40}]}\n",
    "                  :mark {:type \"bar\"}\n",
    "                  :encoding {\n",
    "                    :x {:field :mean :type \"quantitative\" :scale {:domain [7 49]}}\n",
    "\n",
    "                  }\n",
    "                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question breakdown\n",
    "Across all the responses, on average, the **worst scoring questions were**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul></ul>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def ps-question-stats (get psych-safety-results :question-stats))\n",
    "(defn ps-question->score [q] (get-in ps-question-stats [:question-stats q]))\n",
    "\n",
    "(defn score->answer\n",
    "    [score qid]\n",
    "    (let [q-scores (:scoring (get model-index qid))\n",
    "          scale (:scale (get model-index qid))\n",
    "          scaled-score (scale score q-scores)]\n",
    "        (some (fn [[answer qscore]] \n",
    "                  (when (= qscore scaled-score) answer)) \n",
    "              q-scores)))\n",
    "\n",
    "(def worst-3-questions\n",
    "    (->> (get ps-question-stats :worst-questions)\n",
    "         (take 3)\n",
    "         (map #(assoc (ps-question->score %) :id %))))\n",
    "\n",
    "(defn render-question [qs]\n",
    "    (let [mean (:score-mean qs)\n",
    "          stddev (:score-stddev qs)\n",
    "          cl (int (Math/ceil mean))\n",
    "          cl-answer (score->answer cl (:id qs))\n",
    "          fl (int (Math/floor mean))\n",
    "          fl-answer (score->answer fl (:id qs))\n",
    "          ]\n",
    "     [:li \"\\\"\" (:id qs) \"\\\"\" \n",
    "      [:ul \n",
    "       [:li \"Mean score: \" (reports/fmt-number mean) \" (stddev: \" (reports/fmt-number stddev) \").\"]\n",
    "       [:li \"which is between \\\"\" fl-answer \"/\" fl \"\\\" and \\\"\" cl-answer \"/\" cl \"\\\".\"]]])\n",
    "    )\n",
    "\n",
    "(hiccup-html \n",
    "    [:ul (map render-question worst-3-questions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the responses, on average, the **best scoring questions were**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul></ul>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def best-3-questions\n",
    "    (->> (get ps-question-stats :best-questions)\n",
    "         (take 3)\n",
    "         (map #(assoc (ps-question->score %) :id %))))\n",
    "\n",
    "(hiccup-html \n",
    "    [:ul (map render-question best-3-questions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "When analyzing psychological safety, it is important to look at outliers as well as averages: It is important that *everyone* on the team feel psychologically safe - if that is not the case, we cannot consider the environment fully inclusive, nor can we expect it to bring out the best in everyone on the team.\n",
    "\n",
    "\n",
    "### PS-scores: the distribution\n",
    "This is the distribution of PS scores (normalized to a 1-7 scale (as opposed to 7-49))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v3+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.json",
       "data": {
        "values": []
       },
       "description": "Distribution of psych safety scores",
       "encoding": {
        "x": {
         "field": "index",
         "title": "Response number",
         "type": "ordinal"
        },
        "y": {
         "field": "mean-score",
         "scale": {
          "domain": [
           1,
           7
          ]
         },
         "title": "Mean PS score (1-7)",
         "type": "quantitative"
        }
       },
       "mark": "point"
      },
      "image/png": "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"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def psych-safety-response-stats (get psych-safety-results :response-stats))\n",
    ";;(render-mime \"application/json\" psych-safety-response-stats)\n",
    "\n",
    "(render-mime \"application/vnd.vegalite.v3+json\" \n",
    "             {\n",
    "                  :$schema \"https://vega.github.io/schema/vega-lite/v4.json\",\n",
    "                  :description \"Distribution of psych safety scores\"\n",
    "                  :data {:values (map-indexed (fn [i x] (assoc x :index i)) \n",
    "                                              (:response-mean-scores psych-safety-response-stats))}\n",
    "                  :mark \"point\"\n",
    "                  :encoding {\n",
    "                    :y {:field :mean-score :type \"quantitative\" :scale {:domain [1 7]} :title \"Mean PS score (1-7)\"}\n",
    "                    :x {:field :index :type \"ordinal\" :title \"Response number\"}\n",
    "                  }\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are a people on the team who perceive the environment as less psychologically safe than others. The five worsts scores are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(->> (get-in psych-safety-results [:response-stats :worst-scores])\n",
    "     (take 5)\n",
    "     (map :mean-score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question breakdown for outliers\n",
    "Let's do a breakdown of scores by question for this group scoring lowest on PS.\n",
    "\n",
    "Across the five worst-scoring responses, the **worst scoring questions were**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul></ul>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def worst-5-outlier-responses (set (reports/take-outliers 5 :worst-scores psych-safety-results)))\n",
    "(def worst-5-outlier-scored-responses (filter #(worst-5-outlier-responses (:responseId %)) scored-responses))\n",
    "\n",
    "(def worst-5-outlier-psych-safety-results \n",
    "    (get (analysis/aggregate-scores example-model worst-5-outlier-scored-responses)\n",
    "         \"Psychological safety\"))\n",
    "\n",
    "(def worst-5-ps-question-stats (get worst-5-outlier-psych-safety-results :question-stats))\n",
    "(defn worst-5-ps-question->score [q] (get-in worst-5-ps-question-stats [:question-stats q]))\n",
    "\n",
    "(def worst-5-outliers-worst-3-questions\n",
    "    (->> (get worst-5-ps-question-stats :worst-questions)\n",
    "         (take 3)\n",
    "         (map #(assoc (worst-5-ps-question->score %) :id %))))\n",
    "\n",
    "(hiccup-html \n",
    "    [:ul (map render-question worst-5-outliers-worst-3-questions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the five worst-scoring responses, the **best scoring questions were**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul></ul>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def worst-5-outliers-best-3-questions\n",
    "    (->> (get worst-5-ps-question-stats :best-questions)\n",
    "         (take 3)\n",
    "         (map #(assoc (worst-5-ps-question->score %) :id %))))\n",
    "\n",
    "(hiccup-html \n",
    "    [:ul (map render-question worst-5-outliers-best-3-questions)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psychological safety - domains\n",
    "The degree of psychological safety in a situation can depend on the topic being discussed. It is possible that some topics are perceived as safe whereas others are considered unsafe. To better understand team sentiment, we asked each respondent to \"... choose the three topics which you find the most uncomfortable, or where you'd least want to bring up your concerns or have a conversation about.\"\n",
    "\n",
    "The participants picked among the following options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul></ul>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def domains-map   {\"Giving or receiving critical feedback\" :feedback\n",
    "                    \"Asking potentially dumb/newbie questions\" :questions\n",
    "                    \"Disagreeing or voicing alternative opinions in team meetings\" :meetings\n",
    "                    \"Disagreeing with specific people (\\\"oh, no one argues with them\\\")\" :specific-people\n",
    "                    \"Rebalancing your workload (delegating or asking for support)\" :workload\n",
    "                    \"Contributing as a more junior team member or one with different credentials\" :credentials\n",
    "                    \"Understanding what it takes to get promoted (or why one person was promoted over another)\" :promos\n",
    "                    \"Raising taboo topics (i.e., not up for discussion without invoking very strong pro or con feelings). Please specify the topic(s) under \\\"Other\\\".\" :taboo})\n",
    "\n",
    "(def domains-inverse-map (zipmap (vals domains-map) (keys domains-map)))\n",
    "\n",
    "(def domains-question \"For you personally, choose the three topics which you find the most uncomfortable, or where you'd least want to bring up your concerns or have a conversation about.\")\n",
    "\n",
    "(defn parse-domains [answer]\n",
    "    (->> domains-map\n",
    "         (map (fn [[a k]]                   \n",
    "                  [k (.indexOf answer a)])) ;; find index of each possible answer\n",
    "         (filter (fn [[k index]] (>= index 0)))\n",
    "         (map first)))\n",
    "    \n",
    "\n",
    "(def domains-answers-counts \n",
    "    (->> scored-responses ;; all responses\n",
    "        (map #(get-in % [:groups domains-question])) ;; get the responses to the domain-question\n",
    "        (remove nil?)\n",
    "        (map parse-domains) ;; [ (:a :b :c) (:a :b :d) (:d :e :f) ]\n",
    "        (map frequencies) ;; [{:a 1 :b 1 :c 1} {:a 1 :b 1 :d 1} {:d 1 :e 1 :f 1}]\n",
    "        (apply (partial merge-with +));; {:a 2 :b 2 :c 1 :d 1 :e 1 :f 1}\n",
    "        (sort-by second) ;; ([:f 1] [:e 1] [:d 1] [:c 1] [:b 2] [:a 2])\n",
    "        reverse))\n",
    "\n",
    "(hiccup-html \n",
    "    [:ul (map (fn [[key count]]\n",
    "                  [:li (get domains-inverse-map key) \": \" count \" votes.\"]\n",
    "                  )\n",
    "              domains-answers-counts)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Open-ended feedback\n",
    "We also asked more open-ended questions, allowing people to provide verbatim responses. In these questions, we asked “If you could change one team process or one thing about how the team works, what would you change and why?” and “What is something the team does really well? Something that would make you very disappointed if the team stopped doing or something that makes you proud to be part of the team.”\n",
    "\n",
    "\n",
    "### Q: \"If you could change one team process, or one thing about how the team works, what would you change and why?\"\n",
    "Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul></ul>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def open-ended-verbatims (:verbatims (get aggregate-results \"Open-ended feedback\")))\n",
    "\n",
    "(def could-be-improved (get open-ended-verbatims \"If you could change one team process, or one thing about how the team works, what would you change and why?\"))\n",
    "\n",
    "(hiccup-html \n",
    "    [:ul (map #(do [:li \"\\\"\" % \".\\\"\"]) could-be-improved)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: \"If you could change one team process, or one thing about how the team works, what would you change and why?\"\n",
    "Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul></ul>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def working-well (get open-ended-verbatims \"What is something the team does really well? Something that would make you very disappointed if the team stopped doing or something that makes you proud to be part of the team.\"))\n",
    "(hiccup-html \n",
    "    [:ul (map #(do [:li \"\\\"\" % \".\\\"\"]) working-well)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have feedback?\n",
    "Please send to @krukow / krukow@github.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clojure (edmondson)",
   "language": "clojure",
   "name": "edmondson"
  },
  "language_info": {
   "file_extension": ".clj",
   "mimetype": "text/x-clojure",
   "name": "clojure",
   "version": "1.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
